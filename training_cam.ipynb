{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "aGeb369YELIU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LABEL_NAMES = ['scolomfr-voc-015-num-1179',\n",
    " 'scolomfr-voc-015-num-1548',\n",
    " 'scolomfr-voc-015-num-1032',\n",
    " 'scolomfr-voc-015-num-1333',\n",
    " 'scolomfr-voc-015-num-6360',\n",
    " 'scolomfr-voc-015-num-980',\n",
    " 'scolomfr-voc-015-num-1430',\n",
    " 'scolomfr-voc-015-num-919',\n",
    " 'scolomfr-voc-015-num-7755',\n",
    " 'scolomfr-voc-015-num-1831',\n",
    " 'scolomfr-voc-015-num-6364',\n",
    " 'scolomfr-voc-015-num-1832',\n",
    " 'scolomfr-voc-015-num-6365',\n",
    " 'scolomfr-voc-015-num-1834',\n",
    " 'scolomfr-voc-015-num-6369',\n",
    " 'scolomfr-voc-015-num-7816']\n",
    "\n",
    "DATA_PATH = '/content/drive/MyDrive/domain-helper/'\n",
    "LOG_PATH = '/content/drive/MyDrive/domain-helper/logs/'\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "!pip install Sentencepiece\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install nlp\n",
    "\n",
    "from transformers import CamembertForSequenceClassification, CamembertTokenizerFast, Trainer, TrainingArguments\n",
    "import torch\n",
    "from nlp import load_dataset, DatasetDict\n",
    "\n",
    "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
    "camembert = CamembertForSequenceClassification.from_pretrained(\"camembert-base\", num_labels=len(LABEL_NAMES))\n",
    "\n",
    "dataset = load_dataset('csv', data_files=[str(DATA_PATH) + '/edubases_domain_labeled_data.csv',str(DATA_PATH) + '/gar_domain_labeled_data.csv'], split='train' )\n",
    "\n",
    "# 90% train, 10% test + validation\n",
    "train_test_valid = dataset.train_test_split(test_size=0.1)\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_test_valid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "train_test_valid_dataset = DatasetDict({\n",
    "    'train': train_test_valid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "\n",
    "train_set = train_test_valid_dataset['train']\n",
    "test_set = train_test_valid_dataset['test']\n",
    "validation_set = train_test_valid_dataset['valid']\n",
    "\n",
    "def preprocess(data):\n",
    "    return tokenizer(data['title'], padding=True, truncation=True)\n",
    "\n",
    "train_set = train_set.map(preprocess, batched=True,\n",
    "                          batch_size=len(train_set))\n",
    "test_set = test_set.map(preprocess, batched=True, batch_size=len(test_set))\n",
    "\n",
    "train_set.set_format('torch',\n",
    "                      columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_set.set_format('torch',\n",
    "                     columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 4\n",
    "\n",
    "warmup_steps = 500\n",
    "weight_decay = 0.01\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'{DATA_PATH}/results',\n",
    "    label_names=LABEL_NAMES,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    evaluation_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=camembert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ZxhXXPbVgSsQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y_test = validation_set[\"label\"]\n",
    "X_test_tokenized = tokenizer(validation_set[\"title\"], padding=True, truncation=True)\n",
    "\n",
    "!pip install sklearn\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "\n",
    "validation_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "# Make prediction\n",
    "raw_pred, a, b = trainer.predict(validation_dataset)\n",
    "\n",
    "# Preprocess raw predictions\n",
    "Y_pred = np.argmax(raw_pred, axis=1)\n",
    "\n",
    "print(precision_score(Y_test, Y_pred, average='micro'))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "4ZmfrE_RgSsX"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "colab": {
   "name": "training_cam.ipynb",
   "provenance": [],
   "machine_shape": "hm",
   "collapsed_sections": [],
   "background_execution": "on"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}